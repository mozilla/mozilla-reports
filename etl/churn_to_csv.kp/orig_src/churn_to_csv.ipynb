{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Churn to CSV\"\n",
    "authors:\n",
    "- amiyaguchi\n",
    "tags:\n",
    "- churn\n",
    "- etl\n",
    "- csv\n",
    "created_at: 2016-03-07\n",
    "updated_at: 2016-03-07\n",
    "tldr: Convert telemetry-parquet/churn to csv\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn to CSV\n",
    "\n",
    "[Bug 1345217](https://bugzilla.mozilla.org/show_bug.cgi?id=1345217)\n",
    "\n",
    "This script turns the parquet dataset generated by [churn notebook](https://github.com/mozilla/mozilla-reports/blob/master/etl/churn.kp/knowledge.md) into csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import gzip\n",
    "\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def csv(f):\n",
    "    return \",\".join([unicode(a) for a in f])\n",
    "\n",
    "def fmt(d, date_format=\"%Y%m%d\"):\n",
    "    return datetime.strftime(d, date_format)\n",
    "\n",
    "def collect_and_upload_csv(df, filename, upload_config):\n",
    "    \"\"\" Collect the dataframe into a csv file and upload to target locations. \"\"\"\n",
    "    client = boto3.client('s3', 'us-west-2')\n",
    "    transfer = S3Transfer(client)\n",
    "\n",
    "    print(\"{}: Writing output to {}\".format(datetime.utcnow(), filename))\n",
    "\n",
    "    # Write the file out as gzipped csv\n",
    "    with gzip.open(filename, 'wb') as fout:\n",
    "        fout.write(\",\".join(df.columns) + \"\\n\")\n",
    "        print(\"{}: Wrote header to {}\".format(datetime.utcnow(), filename))\n",
    "        records = df.rdd.collect()\n",
    "        for r in records:\n",
    "            try:\n",
    "                fout.write(csv(r))\n",
    "                fout.write(\"\\n\")\n",
    "            except UnicodeEncodeError as e:\n",
    "                print(\"{}: Error writing line: {} // {}\".format(datetime.utcnow(), e, r))\n",
    "        print(\"{}: finished writing lines\".format(datetime.utcnow()))\n",
    "\n",
    "    # upload files to s3\n",
    "    try: \n",
    "        for config in upload_config:\n",
    "            print(\"{}: Uploading to {} at s3://{}/{}/{}\".format(\n",
    "                    datetime.utcnow(), config[\"name\"], config[\"bucket\"], \n",
    "                    config[\"prefix\"], filename))\n",
    "\n",
    "            s3_path = \"{}/{}\".format(config[\"prefix\"], filename)\n",
    "            transfer.upload_file(filename, config[\"bucket\"], s3_path,\n",
    "                                 extra_args={'ACL': 'bucket-owner-full-control'})\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(\"File for {} already exists, skipping upload: {}\".format(filename, e))\n",
    "\n",
    "\n",
    "def marginalize_dataframe(df):\n",
    "    \"\"\" Reduce the granularity of the dataset to the original set of attributes.\n",
    "    The original set of attributes can be found on commit 2de3ef1 of mozilla-reports. \"\"\"\n",
    "    \n",
    "    attributes = ['channel', 'geo', 'is_funnelcake',\n",
    "                  'acquisition_period', 'start_version', 'sync_usage',\n",
    "                  'current_version', 'current_week', 'is_active']\n",
    "    aggregates = ['n_profiles', 'usage_hours', 'sum_squared_usage_hours']\n",
    "    \n",
    "    return df.groupby(attributes).agg(*[F.sum(x).alias(x) for x in aggregates])\n",
    "\n",
    "\n",
    "def convert_week(config, week_start=None):\n",
    "    \"\"\" Convert a given retention period from parquet to csv. \"\"\"\n",
    "    df = spark.read.parquet(config[\"source\"])\n",
    "    \n",
    "    # find the latest start date based on the dataset if not provided\n",
    "    if not week_start:\n",
    "        start_dates = df.select(\"week_start\").distinct().collect()\n",
    "        week_start = sorted(start_dates)[-1].week_start\n",
    "    \n",
    "    # find the week end for the filename\n",
    "    week_end = fmt(datetime.strptime(week_start, \"%Y%m%d\") + timedelta(6))\n",
    "    \n",
    "    print(\"Running for the week of {} to {}\".format(week_start, week_end))\n",
    "    \n",
    "    # find the target subset of data\n",
    "    df = df.where(df.week_start == week_start)\n",
    "    \n",
    "    # marginalize the dataframe to the original attributes and upload to s3\n",
    "    upload_df = marginalize_dataframe(df)\n",
    "    filename = \"churn-{}-{}.by_activity.csv.gz\".format(week_start, week_end)\n",
    "    collect_and_upload_csv(upload_df, filename, config[\"uploads\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assert_valid_config(config):\n",
    "    \"\"\" Assert that the configuration looks correct. \"\"\"\n",
    "    assert set([\"source\", \"uploads\"]).issubset(config.keys())\n",
    "    for entry in config[\"uploads\"]:\n",
    "        assert set([\"name\", \"bucket\", \"prefix\"]).issubset(entry.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from moztelemetry.standards import snap_to_beginning_of_week\n",
    "from os import environ\n",
    "\n",
    "config = {\n",
    "    \"source\": \"s3://telemetry-parquet/churn/v2\",\n",
    "    \"uploads\": [\n",
    "        {\n",
    "            \"name\":   \"Pipeline-Analysis\",\n",
    "            \"bucket\": \"net-mozaws-prod-us-west-2-pipeline-analysis\",\n",
    "            \"prefix\": \"mreid/churn\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":   \"Dashboard\",\n",
    "            \"bucket\": \"net-mozaws-prod-metrics-data\",\n",
    "            \"prefix\": \"telemetry-churn\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Set to True to overwrite the configuration with debugging route\n",
    "if False:\n",
    "    config[\"uploads\"] = [\n",
    "        {\n",
    "            \"name\":   \"Testing\",\n",
    "            \"bucket\": \"net-mozaws-prod-us-west-2-pipeline-analysis\",\n",
    "            \"prefix\": \"amiyaguchi/churn_csv_testing\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# check for a date, in the case of a backfill\n",
    "env_date = environ.get('date')\n",
    "week_start = None\n",
    "if env_date:\n",
    "    # Churn waits 10 days for pings to be sent from the client\n",
    "    week_start_date = snap_to_beginning_of_week(\n",
    "        datetime.strptime(env_date, \"%Y%m%d\") - timedelta(10),\n",
    "        \"Sunday\")\n",
    "    week_start = fmt(week_start_date)\n",
    "\n",
    "assert_valid_config(config)\n",
    "convert_week(config, week_start)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}