{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Topline Summary using Main Summary Dataset\n",
    "authors:\n",
    "- acmiyaguchi\n",
    "tags:\n",
    "- main_summary\n",
    "- executive\n",
    "- topline\n",
    "- summary\n",
    "- dataframe\n",
    "- spark\n",
    "- joins\n",
    "created_at: 2016-11-14\n",
    "updated_at: 2016-11-30\n",
    "tldr: |\n",
    "    Port of the topline/executive summary using the main summary dataset (not for production usage).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topline Summary using Main Summary Dataset\n",
    "\n",
    "[Bug 1309574](https://bugzilla.mozilla.org/show_bug.cgi?id=1309574) involves porting the executive summary to use the main summary dataset. The original script can be found [mozilla-services/data-pipeline/run_executive_report.py](https://github.com/mozilla-services/data-pipeline/blob/e5c29541794325388336a210746029dce998b9e5/reports/executive_summary/run_executive_report.py).\n",
    "\n",
    "### Setup and Helper Functions\n",
    "First we define some helper functions that will serve us later. This report can be done on a weekly and monthly basis, so I have included a flexible date function for manipulating the report period. There are also serveral pyspark user defined functions (UDF) for preprocessing data since many of the fields are not within expected range of values.\n",
    "\n",
    "### Sourcing the Data\n",
    "This defines the sources of data used for the executive summary and the dataframes that represent them. We load the main summary from telemetry-parquet. The crash data needs to be sourced from raw pings, but the information can be easily joined with the summarized data.\n",
    "\n",
    "### Queries\n",
    "These queries translate the original sql queries into their equivalent spark dataframe operations. I have opted to avoid the use of spark's ability to run raw sql statements since there wasn't a direct mapping from the original script to this notebook. In particular, exploding the search_counts field was less than straightforward.\n",
    "\n",
    "### Execution\n",
    "Finally, the entry point for execution can be found near the end of the notebook. This is where the report date and period can be manipulated to generate a new dataset for the executive report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install arrow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# reduce noise\n",
    "logging.getLogger('boto3').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('botocore').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes a report start date, and will generate a report monthly or weekly. We need to get the week or month following the start date. If we are determining the count of inactive users, we will need to query in the [previous period](https://github.com/mozilla-services/data-pipeline/blob/master/reports/executive_summary/run_executive_report.py#L163)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import arrow\n",
    "\n",
    "def date_range(start_date, period, last_period=False):\n",
    "    \"\"\"Returns the begin_date, end_date for this report.\n",
    "    \n",
    "    start_date:  datestring in the format YYYYMMDD\n",
    "    period:      type of period, either 'months' or 'weeks'\n",
    "    last_period: determines if this will this or last month, defaults to this month\n",
    "    \"\"\"\n",
    "    if period not in ('months', 'weeks', 'day'):\n",
    "        log.warning(\"{} is an invalid argument to date_range, defaulting to `weeks`\"\n",
    "                    .format(period))\n",
    "        period = 'weeks'\n",
    "    \n",
    "    # Not used in production, this is for testing against a smaller subset of days\n",
    "    if period == 'day':\n",
    "        return start_date, start_date\n",
    "\n",
    "    fmt = 'YYYYMMDD'\n",
    "    begin_date = arrow.get(start_date, fmt)\n",
    "    if last_period:\n",
    "        # offset the start date by a single period\n",
    "        offset = {period: -1}\n",
    "        begin_date = begin_date.replace(**offset)\n",
    "\n",
    "    offset = {period: 1}\n",
    "    end_date = begin_date.replace(**offset)\n",
    "    \n",
    "    return begin_date.format(fmt), end_date.format(fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions\n",
    "These UDFs capture the logic described in the [`extract_executive_summary.lua` heka decoder]( https://github.com/mozilla-services/data-pipeline/blob/515b85101e7335a66fb8c26316cd9721c832b098/heka/sandbox/decoders/extract_executive_summary.lua)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "SEC_IN_HOUR = 60 * 60\n",
    "SEC_IN_DAY = SEC_IN_HOUR * 24\n",
    "\n",
    "# UDF for aggregating search counts based on search engine\n",
    "\n",
    "search_schema = StructType([\n",
    "    StructField(\"google\", IntegerType(), False),\n",
    "    StructField(\"bing\", IntegerType(), False),\n",
    "    StructField(\"yahoo\", IntegerType(), False),\n",
    "    StructField(\"other\", IntegerType(), False)])\n",
    "\n",
    "search_pat = ['[Gg]oogle', '[Bb]ing', '[Yy]ahoo', '.']\n",
    "search_pat_comp = [re.compile(pat) for pat in search_pat]\n",
    "\n",
    "def get_search_counts(row):\n",
    "    \"\"\" Return a list of aggregate values for each search engine in a row. \"\"\"\n",
    "    counts = [0, 0, 0, 0]\n",
    "\n",
    "    # This client does not have any searches\n",
    "    if not row:\n",
    "        return counts\n",
    "\n",
    "    for cell in row:\n",
    "        if not cell.engine:\n",
    "            continue\n",
    "        for i, regex in enumerate(search_pat_comp):\n",
    "            if regex.match(cell.engine):\n",
    "                counts[i] += cell['count'] or 0\n",
    "    return counts\n",
    "\n",
    "\n",
    "# UDF for determining hours of usage\n",
    "\n",
    "def get_hours(uptime):\n",
    "    \"\"\" Convert uptime from seconds to hours if the uptime value is plausible. \"\"\"\n",
    "    if not uptime or uptime < 0 or uptime >= 180 * SEC_IN_DAY:\n",
    "        return 0.0\n",
    "    return uptime / SEC_IN_HOUR\n",
    "\n",
    "\n",
    "# UDF for determining when the client profile was created\n",
    "\n",
    "def get_profile_creation(timestamp):\n",
    "    \"\"\"Convert days since unix epoch to nanoseconds since epoch\"\"\"\n",
    "    if not timestamp or timestamp < 0:\n",
    "        return 0\n",
    "    return timestamp * SEC_IN_DAY\n",
    "\n",
    "\n",
    "# UDF for normalizing country name\n",
    "\n",
    "country_names = set([\n",
    "    \"AD\",\"AE\",\"AF\",\"AG\",\"AI\",\"AL\",\"AM\",\"AO\",\"AQ\",\"AR\",\"AS\",\"AT\",\"AU\",\n",
    "    \"AW\",\"AX\",\"AZ\",\"BA\",\"BB\",\"BD\",\"BE\",\"BF\",\"BG\",\"BH\",\"BI\",\"BJ\",\"BL\",\"BM\",\"BN\",\n",
    "    \"BO\",\"BQ\",\"BR\",\"BS\",\"BT\",\"BV\",\"BW\",\"BY\",\"BZ\",\"CA\",\"CC\",\"CD\",\"CF\",\"CG\",\"CH\",\n",
    "    \"CI\",\"CK\",\"CL\",\"CM\",\"CN\",\"CO\",\"CR\",\"CU\",\"CV\",\"CW\",\"CX\",\"CY\",\"CZ\",\"DE\",\"DJ\",\n",
    "    \"DK\",\"DM\",\"DO\",\"DZ\",\"EC\",\"EE\",\"EG\",\"EH\",\"ER\",\"ES\",\"ET\",\"FI\",\"FJ\",\"FK\",\"FM\",\n",
    "    \"FO\",\"FR\",\"GA\",\"GB\",\"GD\",\"GE\",\"GF\",\"GG\",\"GH\",\"GI\",\"GL\",\"GM\",\"GN\",\"GP\",\"GQ\",\n",
    "    \"GR\",\"GS\",\"GT\",\"GU\",\"GW\",\"GY\",\"HK\",\"HM\",\"HN\",\"HR\",\"HT\",\"HU\",\"ID\",\"IE\",\"IL\",\n",
    "    \"IM\",\"IN\",\"IO\",\"IQ\",\"IR\",\"IS\",\"IT\",\"JE\",\"JM\",\"JO\",\"JP\",\"KE\",\"KG\",\"KH\",\"KI\",\n",
    "    \"KM\",\"KN\",\"KP\",\"KR\",\"KW\",\"KY\",\"KZ\",\"LA\",\"LB\",\"LC\",\"LI\",\"LK\",\"LR\",\"LS\",\"LT\",\n",
    "    \"LU\",\"LV\",\"LY\",\"MA\",\"MC\",\"MD\",\"ME\",\"MF\",\"MG\",\"MH\",\"MK\",\"ML\",\"MM\",\"MN\",\"MO\",\n",
    "    \"MP\",\"MQ\",\"MR\",\"MS\",\"MT\",\"MU\",\"MV\",\"MW\",\"MX\",\"MY\",\"MZ\",\"NA\",\"NC\",\"NE\",\"NF\",\n",
    "    \"NG\",\"NI\",\"NL\",\"NO\",\"NP\",\"NR\",\"NU\",\"NZ\",\"OM\",\"PA\",\"PE\",\"PF\",\"PG\",\"PH\",\"PK\",\n",
    "    \"PL\",\"PM\",\"PN\",\"PR\",\"PS\",\"PT\",\"PW\",\"PY\",\"QA\",\"RE\",\"RO\",\"RS\",\"RU\",\"RW\",\"SA\",\n",
    "    \"SB\",\"SC\",\"SD\",\"SE\",\"SG\",\"SH\",\"SI\",\"SJ\",\"SK\",\"SL\",\"SM\",\"SN\",\"SO\",\"SR\",\"SS\",\n",
    "    \"ST\",\"SV\",\"SX\",\"SY\",\"SZ\",\"TC\",\"TD\",\"TF\",\"TG\",\"TH\",\"TJ\",\"TK\",\"TL\",\"TM\",\"TN\",\n",
    "    \"TO\",\"TR\",\"TT\",\"TV\",\"TW\",\"TZ\",\"UA\",\"UG\",\"UM\",\"US\",\"UY\",\"UZ\",\"VA\",\"VC\",\"VE\",\n",
    "    \"VG\",\"VI\",\"VN\",\"VU\",\"WF\",\"WS\",\"YE\",\"YT\",\"ZA\",\"ZM\",\"ZW\"])\n",
    "\n",
    "def normalize_country(name):\n",
    "    if not name or type(name) != str:\n",
    "        return \"Other\"\n",
    "        \n",
    "    name = name.upper()\n",
    "    if name not in country_names:\n",
    "        return \"Other\"\n",
    "    return name\n",
    "\n",
    "\n",
    "# Information relevant to creating a UDF to normalize channel column\n",
    "channel_labels = ['release', 'beta', 'nightly', 'aurora', 'Other']\n",
    "channel_pat = [\n",
    "    '^release$',\n",
    "    '^beta$', \n",
    "    '^nightly$|^nightly-cck-', \n",
    "    '^aurora$',\n",
    "    '.'\n",
    "]\n",
    "channel_pat_comp = [re.compile(pat) for pat in channel_pat]\n",
    "\n",
    "\n",
    "# Information relevant to creating a UDF to normalize os column+\n",
    "os_labels = ['Windows', 'Mac', 'Linux', 'Other']\n",
    "os_pat = [\n",
    "    '^Windows|WINNT', \n",
    "    'Darwin', \n",
    "    'Linux|BSD|SunOS', \n",
    "    '.'\n",
    "]\n",
    "os_pat_comp = [re.compile(pat) for pat in os_pat]\n",
    "\n",
    "\n",
    "def normalize(patterns, labels, s):\n",
    "    \"\"\" Categorize a string s based on whether it matches a compiled regex pattern.\n",
    "    The default pattern should be at index -1.\n",
    "    \n",
    "    pattern: list of compiled regex pattern to match against\n",
    "    labels:  list of lables of len(pattern), where label[i] corresponds to pattern[i]\n",
    "    s:       string to categorize\n",
    "    \"\"\"\n",
    "    \n",
    "    norm = labels[-1]\n",
    "    if not s:\n",
    "        return norm\n",
    "    for label, pattern in zip(labels, patterns):\n",
    "        if pattern.match(s):\n",
    "            norm = label\n",
    "            break\n",
    "    return norm\n",
    "\n",
    "\n",
    "# declare the User Defined Functions for usage later\n",
    "search_udf = udf(get_search_counts, search_schema)\n",
    "hour_udf = udf(get_hours, DoubleType())\n",
    "creation_udf = udf(get_profile_creation, IntegerType())\n",
    "country_udf = udf(normalize_country, StringType())\n",
    "channel_udf = udf(partial(normalize, channel_pat_comp, channel_labels), StringType())\n",
    "os_udf = udf(partial(normalize, os_pat_comp, os_labels), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sourcing the Data\n",
    "\n",
    "The report dataframe is a subset of the `main_summary` that is relevant to our summaries. We also generate a crash dataframe to count the number of crashes based on country, channel, and operating system.\n",
    "\n",
    "The relevant columns were found by cross referencing `run_executive_report.py` with columns generated by the `extract_executive_summary.lua` filter.\n",
    "\n",
    "### main_summary\n",
    "The [MainSummaryExample notebook](https://gist.github.com/mreid-moz/518f7515aac54cd246635c333683ecce) is good starting place for understanding how to use the main_summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SQLContext, Window\n",
    "\n",
    "\n",
    "def get_report_df(start_date, end_date):\n",
    "    \"\"\"Return a dataframe containing the raw report data. \"\"\"\n",
    "    \n",
    "    source_url = \"s3://telemetry-parquet/main_summary/v3\"\n",
    "    main_summary_df = sqlContext.read.parquet(source_url)\n",
    "\n",
    "    columns = [\n",
    "        'client_id',\n",
    "        country_udf('country').alias('country'),\n",
    "        'submission_date',\n",
    "        'is_default_browser',\n",
    "        creation_udf('profile_creation_date').alias('profile_creation_date'), \n",
    "        channel_udf('channel').alias('channel'), \n",
    "        os_udf('os').alias('os'),\n",
    "        search_udf('search_counts').alias('search_counts'),\n",
    "        hour_udf(F.col('subsession_length').cast('double')).alias('hours')\n",
    "    ]\n",
    "\n",
    "    report_df = (\n",
    "        main_summary_df\n",
    "            .filter(start_date <= F.col('submission_date_s3'))\n",
    "            .filter(F.col('submission_date_s3') <= end_date)\n",
    "            .filter(F.col('app_name') == 'Firefox')\n",
    "            .select(columns)\n",
    "    )\n",
    "    \n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### telemetry-pings where docType = 'crash'\n",
    "We are missing the crash pings in the main summary, but we can filter the raw pings manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from moztelemetry import get_pings_properties\n",
    "from moztelemetry.dataset import Dataset\n",
    "\n",
    "\n",
    "def get_crash_df(start_date, end_date):\n",
    "    \"\"\" Return a dataframe containing relevent columns from crash pings. \"\"\"t\n",
    "    ping_rdd = (\n",
    "        Dataset\n",
    "            .from_source(\"telemetry\")\n",
    "            .where(docType='crash')\n",
    "            .where(appName='Firefox')\n",
    "            .where(submissionDate=lambda x: start_date <= x <= end_date)\n",
    "            .records(sc)\n",
    "    )\n",
    "\n",
    "    crash_rdd = get_pings_properties(ping_rdd, {\n",
    "            \"country\": \"meta/geoCountry\", \n",
    "            \"channel\": \"meta/appUpdateChannel\",\n",
    "            \"os\": \"meta/os\"\n",
    "        })\n",
    "\n",
    "    crash_schema = StructType([\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"os\", StringType(), True),t\n",
    "            StructField(\"channel\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    # this guarantees the order in the crash_schema\n",
    "    def to_tuple(row):\n",
    "        return row['country'], row['os'], row['channel']\n",
    "\n",
    "    return sqlContext.createDataFrame(crash_rdd.map(to_tuple), crash_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_easy_aggregates(report_df, crash_df, groups, report_date):\n",
    "    \"\"\" Return the aggregates of the number of hours, crashes, and search counts. \"\"\"\n",
    "    \n",
    "    # Clean the columns of the crash aggregate to match the easy ones\n",
    "    crash_agg_df = (\n",
    "        crash_df\n",
    "            .select(\n",
    "                country_udf('country').alias('country'),\n",
    "                channel_udf('channel').alias('channel'), \n",
    "                os_udf('os').alias('os'))\n",
    "            .groupby(groups) \n",
    "            .agg(F.count('*').alias('crashes'))\n",
    "    )\n",
    "\n",
    "    # Join the hours and search aggregates with the crash aggregates\n",
    "    easy_aggregate_df = (\n",
    "        report_df\n",
    "            .groupby(groups)\n",
    "            .agg(\n",
    "                F.sum('hours').alias('hours'), \n",
    "                F.sum('search_counts.google').alias('google'),\n",
    "                F.sum('search_counts.bing').alias('bing'),\n",
    "                F.sum('search_counts.yahoo').alias('yahoo'),\n",
    "                F.sum('search_counts.other').alias('other')) \n",
    "            .join(crash_agg_df, ['country', 'channel', 'os', 'date'])\n",
    "    )\n",
    "    \n",
    "    return easy_aggregate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_client_values(report_df, groups, report_date):\n",
    "    \"\"\" Return the number of new clients, default clients, and active users. \"\"\"\n",
    "    \n",
    "    # Used to determine new clients in the report period\n",
    "    report_timestamp = arrow.get(report_date, 'YYYYMMDD').timestamp\n",
    "\n",
    "    # First find the most recent value of all the clients.\n",
    "    clients_df = (\n",
    "        report_df\n",
    "            .select(\n",
    "                'client_id', \n",
    "                'country', \n",
    "                'channel', \n",
    "                'os',\n",
    "                F.when(F.col('profile_creation_date') >= report_timestamp, 1)\n",
    "                    .otherwise(0)\n",
    "                    .alias('new_client'),\n",
    "                F.when(F.col('is_default_browser'), 1)\n",
    "                    .otherwise(0)\n",
    "                    .alias('default_client'),\n",
    "                F.row_number()\n",
    "                    .over(Window.partitionBy('client_id')\n",
    "                                .orderBy(F.desc('submission_date')))\n",
    "                    .alias('clientid_rank'))\n",
    "            .select(\n",
    "                'client_id', \n",
    "                'country', \n",
    "                'channel', \n",
    "                'os', \n",
    "                'new_client', \n",
    "                'default_client')\n",
    "            .where(F.col('clientid_rank') == 1)\n",
    "    )\n",
    "\n",
    "    # Find the client's aggregate values\n",
    "    client_values_df = (\n",
    "        clients_df\n",
    "            .groupby(groups)\n",
    "            .agg(\n",
    "                F.count('*').alias('active'),\n",
    "                F.sum('new_client').alias('new_client'),\n",
    "                F.sum('default_client').alias('default_client'))\n",
    "    )\n",
    "    \n",
    "    return client_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Get the initial date in YYYYMMDD format. This is the date format given by airflow,\n",
    "# as well as the date format used to partition the main_summary parquet data\n",
    "report_date = '20160901'\n",
    "start_date, end_date = date_range(report_date, 'weeks')\n",
    "log.info(\"Starting executive report for dates {}-{}\".format(start_date, end_date))\n",
    "\n",
    "# Source the initial data from parquet and raw pings\n",
    "report_df = get_report_df(start_date, end_date)\n",
    "crash_df = get_crash_df(start_date, end_date)\n",
    "\n",
    "report_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# All report data is grouped by and then aggregated over the following columns\n",
    "groups = ['country', 'channel', 'os', F.lit(report_date).alias('date')]\n",
    "\n",
    "# Get the aggregated dataframes\n",
    "easy_aggregates_df = get_easy_aggregates(report_df, crash_df, groups, report_date)\n",
    "client_values_df = get_client_values(report_df, groups, report_date)\n",
    "\n",
    "# Lets join these dataframes together\n",
    "final_report_df = easy_aggregates_df.join(client_values_df, ['country', 'channel', 'os', 'date'])\n",
    "final_report_df.show()\n",
    "\n",
    "log.info(\"Done with processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other code\n",
    "Code that doesn't need to be run during the main notebook execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_date_range():\n",
    "    args = ['20160101', 'months']\n",
    "    start, end = date_range(*args)\n",
    "    assert start == args[0]  # start date is the same\n",
    "    assert start < end       # the end date is in the future\n",
    "    assert end[4:6] == '02'  # the month has changed\n",
    "    \n",
    "    args = ['20160101', 'months', True]\n",
    "    start, end = date_range(*args)\n",
    "    assert end == args[0]       # the start date has become the end date\n",
    "    assert start < end          # the start is still smaller than the end date\n",
    "    assert start[0:4] == '2015' # the year is handled correctly\n",
    "    assert start[4:6] == '12'   # the day is handled correctly\n",
    "    \n",
    "    args = ['20160101', 'weeks']\n",
    "    start, end = date_range(*args)\n",
    "    assert end[6:8] == '08'  # going forward in the week works correctly\n",
    "    \n",
    "test_date_range()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}