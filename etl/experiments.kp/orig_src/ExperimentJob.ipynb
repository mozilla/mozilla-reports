{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Experiment Job\n",
    "authors: \n",
    "- Frank Bertsch \n",
    "tags:\n",
    "- experiment\n",
    "- firefox\n",
    "created_at: 2017-02-01\n",
    "updated_at: 2016-02-08\n",
    "tldr: \"We take all the pings from yesterday, get the information about any experiments: those that started, those running, and those that ended. These are aggregated by channel and outputted to files in s3.\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt, timedelta, date\n",
    "import moztelemetry\n",
    "from os import environ\n",
    "\n",
    "# get the desired target date from the environment, or run\n",
    "# on 'yesterday' by default.\n",
    "yesterday = dt.strftime(dt.utcnow() - timedelta(1), \"%Y%m%d\")\n",
    "target_date = environ.get('date', yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from moztelemetry.dataset import Dataset\n",
    "\n",
    "sample_rate = environ.get('sample', 1)\n",
    "pings = Dataset.from_source(\"telemetry-experiments\") \\\n",
    "                   .where(submissionDate=target_date) \\\n",
    "                   .where(docType=\"main\") \\\n",
    "                   .records(sc, sample=sample_rate) \\\n",
    "                   .filter(lambda x: x.get(\"environment\", {}).get(\"build\", {}).get(\"applicationName\") == \"Firefox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from moztelemetry import get_pings_properties\n",
    "\n",
    "subset = get_pings_properties(pings, {\n",
    "    \"appUpdateChannel\": \"meta/appUpdateChannel\",\n",
    "    \"log\": \"payload/log\",\n",
    "    \"activeExperiment\": \"environment/addons/activeExperiment/id\",\n",
    "    \"activeExperimentBranch\": \"environment/addons/activeExperiment/branch\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "### Setup data structures and constants ###\n",
    "\n",
    "ALLOWED_ENTRY_TYPES = ('EXPERIMENT_ACTIVATION', 'EXPERIMENT_TERMINATION')\n",
    "\n",
    "experiment = {\n",
    "    'EXPERIMENT_ACTIVATION': defaultdict(int), \n",
    "    'active': defaultdict(int), \n",
    "    'EXPERIMENT_TERMINATION': defaultdict(int)\n",
    "}\n",
    "\n",
    "channel = { \n",
    "    'errors': [], \n",
    "    'experiments': {}\n",
    "}\n",
    "\n",
    "def get_empty_channel():\n",
    "    return deepcopy(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson\n",
    "import requests\n",
    "\n",
    "# This is a json object with {Date => {channel: count}}. It is created\n",
    "# by the main_channel_counts plugin, and may be inaccurate if the ec2\n",
    "# box crashed, but only for the day of the crash. If it crashes, the\n",
    "# previous data will be lost.\n",
    "COUNTS_JSON_URI = \"https://pipeline-cep.prod.mozaws.net/dashboard_output/analysis.frank.main_channel_counts.counts.json\"\n",
    "\n",
    "### Aggregation functions, Spark job, output file creation ###\n",
    "\n",
    "def channel_ping_agg(channel_agg, ping):\n",
    "    \"\"\"Aggregate a channel with a ping\"\"\"\n",
    "    try:\n",
    "        for item in (ping.get(\"log\") or []):\n",
    "            if item[0] in ALLOWED_ENTRY_TYPES:\n",
    "                entry, _, reason, exp_id = item[:4]\n",
    "                data = item[4:]\n",
    "                if exp_id not in channel_agg['experiments']:\n",
    "                    channel_agg['experiments'][exp_id] = deepcopy(experiment)\n",
    "                channel_agg['experiments'][exp_id][entry][tuple([reason] + data)] += 1\n",
    "\n",
    "        exp_id = ping.get(\"activeExperiment\")\n",
    "        branch = ping.get(\"activeExperimentBranch\")\n",
    "        if exp_id is not None and branch is not None:\n",
    "            if exp_id not in channel_agg['experiments']:\n",
    "                channel_agg['experiments'][exp_id] = deepcopy(experiment)\n",
    "            channel_agg['experiments'][exp_id]['active'][branch] += 1\n",
    "    except Exception as e:\n",
    "        channel_agg['errors'].append('{}: {}'.format(e.__class__, str(e)))\n",
    "    \n",
    "    return channel_agg\n",
    "\n",
    "def channel_channel_agg(channel_agg_1, channel_agg_2):\n",
    "    \"\"\"Aggregate a channel with a channel\"\"\"\n",
    "    channel_agg_1['errors'] += channel_agg_2['errors']\n",
    "    \n",
    "    for exp_id, exp in channel_agg_2['experiments'].iteritems():\n",
    "        if exp_id not in channel_agg_1['experiments']:\n",
    "            channel_agg_1['experiments'][exp_id] = deepcopy(experiment)\n",
    "        for entry, exp_activities in exp.iteritems():\n",
    "            for exp_activity, counts in exp_activities.iteritems():\n",
    "                channel_agg_1['experiments'][exp_id][entry][exp_activity] += counts\n",
    "            \n",
    "    return channel_agg_1\n",
    "\n",
    "def get_channel_or_other(ping):\n",
    "    channel = ping.get(\"appUpdateChannel\")\n",
    "    if channel in (\"release\", \"nightly\", \"beta\", \"aurora\"):\n",
    "        return channel\n",
    "    return \"OTHER\"\n",
    "\n",
    "def aggregate_pings(pings):\n",
    "    \"\"\"Get the channel experiments from an rdd of pings\"\"\"\n",
    "    return pings\\\n",
    "            .map(lambda x: (get_channel_or_other(x), x))\\\n",
    "            .aggregateByKey(get_empty_channel(), channel_ping_agg, channel_channel_agg)\n",
    "\n",
    "\n",
    "def add_counts(result):\n",
    "    \"\"\"Add counts from a running CEP\"\"\"\n",
    "    counts = requests.get(COUNTS_JSON_URI).json()\n",
    "    \n",
    "    for cname, channel in result:\n",
    "        channel['total'] = counts.get(target_date, {}).get(cname, None)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def write_aggregate(agg, date, filename_prefix='experiments'):\n",
    "    filenames = []\n",
    "    \n",
    "    for cname, channel in agg:\n",
    "        d = {\n",
    "            \"total\": channel['total'],\n",
    "            \"experiments\": {}\n",
    "        }\n",
    "        for exp_id, experiment in channel['experiments'].iteritems():\n",
    "            d[\"experiments\"][exp_id] = {\n",
    "                \"active\": experiment['active'].items(),\n",
    "                \"activations\": experiment['EXPERIMENT_ACTIVATION'].items(),\n",
    "                \"terminations\": experiment['EXPERIMENT_TERMINATION'].items() \n",
    "            }\n",
    "            \n",
    "        filename = \"{}{}-{}.json.gz\".format(filename_prefix, date, cname)\n",
    "        filenames.append(filename)\n",
    "        \n",
    "        with gzip.open(filename, \"wb\") as fd:\n",
    "            ujson.dump(d, fd)\n",
    "        \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Setup Test Pings ###\n",
    "\n",
    "def make_ping(ae, aeb, chan, log):\n",
    "    return {'activeExperiment': ae,\n",
    "             'activeExperimentBranch': aeb,\n",
    "             'appUpdateChannel': chan,\n",
    "             'log': log}\n",
    "\n",
    "NUM_ACTIVATIONS = 5\n",
    "NUM_ACTIVES = 7\n",
    "NUM_TERMINATIONS = 3\n",
    "TOTAL = NUM_ACTIVATIONS + NUM_ACTIVES + NUM_TERMINATIONS\n",
    "\n",
    "_channel, exp_id, the_date = 'release', 'tls13-compat-ff51@experiments.mozilla.org', '20140101'\n",
    "branch, reason, data = 'branch', 'REJECTED', ['minBuildId']\n",
    "log = [17786, reason, exp_id] + data\n",
    "\n",
    "pings = [make_ping(exp_id, branch, _channel, []) \n",
    "             for i in xrange(NUM_ACTIVES)] +\\\n",
    "        [make_ping(exp_id, branch, _channel, [['EXPERIMENT_ACTIVATION'] + log]) \n",
    "             for i in xrange(NUM_ACTIVATIONS)] +\\\n",
    "        [make_ping(exp_id, branch, _channel, [['EXPERIMENT_TERMINATION'] + log]) \n",
    "             for i in xrange(NUM_TERMINATIONS)]\n",
    "\n",
    "### Setup expected result aggregate ###\n",
    "\n",
    "def channels_agg_assert(channels, counts=1):\n",
    "    #Should just be the channel we provided\n",
    "    assert channels.viewkeys() == set([_channel]), 'Incorrect channels: ' + ','.join(channels.keys())\n",
    "\n",
    "    #just check this one channel now\n",
    "    release = channels[_channel]\n",
    "    assert len(release['errors']) == 0, 'Had Errors: ' + ','.join(release['errors'])\n",
    "\n",
    "    #now check experiment totals\n",
    "    assert release['experiments'][exp_id]['EXPERIMENT_ACTIVATION'][tuple([reason] + data)] == NUM_ACTIVATIONS * counts,\\\n",
    "            'Expected ' + str(NUM_ACTIVATIONS * counts) + \\\n",
    "            ', Got ' + str(release['experiments'][exp_id]['EXPERIMENT_ACTIVATION'][tuple([reason] + data)])\n",
    "    assert release['experiments'][exp_id]['EXPERIMENT_TERMINATION'][tuple([reason] + data)] == NUM_TERMINATIONS * counts,\\\n",
    "            'Expected ' + str(NUM_TERMINATIONS * counts) + \\\n",
    "            ', Got ' + str(release['experiments'][exp_id]['EXPERIMENT_TERMINATION'][tuple([reason] + data)])\n",
    "\n",
    "    #`active` is counted for both just active, and for activations and terminations above\n",
    "    assert release['experiments'][exp_id]['active'][branch] == TOTAL * counts,\\\n",
    "            'Expected ' + str(TOTAL * counts) +\\\n",
    "            'Got ' + str(release['experiments'][exp_id]['active'][branch])\n",
    "    \n",
    "\n",
    "### Test non-spark - easier debugging ###\n",
    "\n",
    "channel_1, channel_2 = get_empty_channel(), get_empty_channel()\n",
    "for ping in pings:\n",
    "    channel_1 = channel_ping_agg(channel_1, ping)\n",
    "    channel_2 = channel_ping_agg(channel_2, ping)\n",
    "\n",
    "# no actual key-value reduce, so just have to add the channel as key\n",
    "res_chan = ((_channel, channel_channel_agg(channel_1, channel_2)),)\n",
    "res_chan = add_counts(res_chan)\n",
    "\n",
    "# we've agggregated over the pings twice, so counts=2\n",
    "channels_agg_assert({channel: agg for channel, agg in res_chan}, counts=2)\n",
    "\n",
    "write_aggregate(res_chan, the_date, filename_prefix=\"nonspark_test\")\n",
    "\n",
    "\n",
    "#### Test Spark ###\n",
    "res = aggregate_pings(sc.parallelize(pings)).collect()\n",
    "res = add_counts(res)\n",
    "\n",
    "channels = {channel: agg for channel, agg in res}\n",
    "\n",
    "channels_agg_assert(channels, counts=1)\n",
    "\n",
    "write_aggregate(res, the_date, filename_prefix=\"spark_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run on actual data - use CEP to get counts ###\n",
    "\n",
    "result = aggregate_pings(subset).collect()\n",
    "result = add_counts(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Upload target day's data files ###\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "\n",
    "output_files = write_aggregate(result, target_date)\n",
    "\n",
    "data_bucket = \"telemetry-public-analysis-2\"\n",
    "s3path = \"experiments/data\"\n",
    "gz_csv_args = {'ContentEncoding': 'gzip', 'ContentType': 'text/csv'}\n",
    "\n",
    "client = boto3.client('s3', 'us-west-2')\n",
    "transfer = S3Transfer(client)\n",
    "\n",
    "for output_file in output_files:\n",
    "    transfer.upload_file(\n",
    "        output_file, \n",
    "        data_bucket, \n",
    "        \"{}/{}\".format(s3path, output_file),\n",
    "        extra_args=gz_csv_args\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}