{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Firefox Destop Churn / Retention Cohort analysis\"\n",
    "authors:\n",
    "- mreid-moz\n",
    "- Dexterp37\n",
    "- acmiyaguchi\n",
    "tags:\n",
    "- churn\n",
    "- retention\n",
    "- cohort\n",
    "- firefox desktop\n",
    "- main_summary\n",
    "created_at: 2016-03-28\n",
    "updated_at: 2016-11-30\n",
    "tldr: |\n",
    "    Compute churn / retention information for unique segments of Firefox \n",
    "    users acquired during a specific period of time.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firefox Destop Churn / Retention Cohort analysis\n",
    "\n",
    "Compute churn / retention information for unique segments of Firefox users acquired during a specific period of time. Tracked in [Bug 1226379](https://bugzilla.mozilla.org/show_bug.cgi?id=1226379). The underlying dataset is generated via the [telemetry-batch-view](https://github.com/mozilla/telemetry-batch-view/blob/master/src/main/scala/com/mozilla/telemetry/views/MainSummaryView.scala) code, and is generated once a day.\n",
    "\n",
    "The aggregated churn data is updated weekly. Due to the client reporting latency, we needs to wait 10 days for the data to stabilize. The report will generate a dataset for the closest sunday before `today - 17 days`. The 17 day figure is calculated by finding the closest day we can aggregate data from (10 days), and then find the closest sunday the week before that (at least 7 days). \n",
    "\n",
    "Code is based on the previous [FHR analysis code](https://github.com/mozilla/churn-analysis).\n",
    "\n",
    "Details and definitions are in [Bug 1198537](https://bugzilla.mozilla.org/show_bug.cgi?id=1198537). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many cores are we running on? \n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read source data\n",
    "\n",
    "Read the data from the parquet datastore on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import ujson as json\n",
    "import requests\n",
    "from datetime import datetime as _datetime, timedelta, date\n",
    "import gzip\n",
    "import boto3\n",
    "import botocore\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "from traceback import print_exc\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from moztelemetry.standards import snap_to_beginning_of_week\n",
    "\n",
    "bucket = \"telemetry-parquet\"\n",
    "prefix = \"main_summary/v3\"\n",
    "s3path = \"s3://{}/{}\".format(bucket, prefix)\n",
    "%time df = spark.read.option(\"mergeSchema\", \"true\").parquet(s3path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_columns = [\n",
    "    \"app_version\",\n",
    "    \"attribution\",\n",
    "    \"channel\",\n",
    "    \"client_id\",\n",
    "    \"country\",\n",
    "    \"default_search_engine\",\n",
    "    \"distribution_id\",\n",
    "    \"locale\",\n",
    "    \"normalized_channel\",\n",
    "    \"profile_creation_date\",\n",
    "    \"submission_date_s3\",\n",
    "    \"subsession_length\",\n",
    "    \"subsession_start_date\",\n",
    "    \"sync_configured\",\n",
    "    \"sync_count_desktop\",\n",
    "    \"sync_count_mobile\",\n",
    "    \"timestamp\"\n",
    "]\n",
    "\n",
    "dataset = df.select(source_columns).withColumnRenamed('app_version', 'version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What do the records look like?\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "\n",
    "Define some helper functions for reorganizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Bug 1289573: Support values like \"mozilla86\" and \"mozilla86-utility-existing\"\n",
    "funnelcake_pattern = re.compile(\"^mozilla[0-9]+.*$\")\n",
    "\n",
    "def get_effective_version(d2v, channel, day):\n",
    "    \"\"\" Get the effective version on the given channel on the given day.\"\"\"\n",
    "    if day not in d2v:\n",
    "        if day < \"2015-01-01\":\n",
    "            return \"older\"\n",
    "        else:\n",
    "            return \"newer\"\n",
    "\n",
    "    effective_version = d2v[day]\n",
    "    return get_channel_version(channel, effective_version)\n",
    "\n",
    "def get_channel_version(channel, version):\n",
    "    \"\"\" Given a channel and an effective release-channel version, give the\n",
    "    calculated channel-specific version.\"\"\"\n",
    "    if channel.startswith('release'):\n",
    "        return version\n",
    "    numeric_version = int(version[0:version.index('.')])\n",
    "    offset = 0\n",
    "    if channel.startswith('beta'):\n",
    "        offset = 1\n",
    "    elif channel.startswith('aurora'):\n",
    "        offset = 2\n",
    "    elif channel.startswith('nightly'):\n",
    "        offset = 3\n",
    "    return \"{}.0\".format(numeric_version + offset)\n",
    "\n",
    "def make_d2v(release_info):\n",
    "    \"\"\" Create a map of yyyy-mm-dd date to the effective Firefox version on the\n",
    "    release channel.\n",
    "    \"\"\"\n",
    "    # Combine major and minor releases into a map of day -> version\n",
    "    # Keep only the highest version available for a day range.\n",
    "    observed_dates = set(release_info[\"major\"].values())\n",
    "    observed_dates |= set(release_info[\"minor\"].values())\n",
    "    # Skip old versions.\n",
    "    sd = [ d for d in sorted(observed_dates) if d >= \"2014-01-01\" ]\n",
    "    start_date_str = sd[0]\n",
    "    start_date = _datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = _datetime.strptime(sd[-1], \"%Y-%m-%d\")\n",
    "    day_count = (end_date - start_date).days + 1\n",
    "    d2v = {}\n",
    "    # Start with all the available version release days:\n",
    "    for t in [\"major\", \"minor\"]:\n",
    "        for m, d in release_info[t].iteritems():\n",
    "            if d < start_date_str:\n",
    "                continue\n",
    "            if d not in d2v or compare_ver(m, d2v[d]) > 0:\n",
    "                d2v[d] = m\n",
    "    last_ver = d2v[start_date_str]\n",
    "    # Fill in all the gaps:\n",
    "    for dt in (start_date + timedelta(n) for n in range(day_count)):\n",
    "        d = _datetime.strftime(dt, \"%Y-%m-%d\")\n",
    "        if d in d2v:\n",
    "            # Don't replace a higher version with a new release of an old\n",
    "            # version (probably an ESR release)\n",
    "            if compare_ver(d2v[d], last_ver) < 0:\n",
    "                d2v[d] = last_ver\n",
    "            else:\n",
    "                last_ver = d2v[d]\n",
    "        else:\n",
    "            d2v[d] = last_ver\n",
    "    return d2v\n",
    "\n",
    "def fetch_json(uri):\n",
    "    \"\"\" Perform an HTTP GET on the given uri, return the results as json. If\n",
    "    there is an error fetching the data, raise it.\n",
    "    \"\"\"\n",
    "    data = requests.get(uri)\n",
    "    # Raise an exception if the fetch failed.\n",
    "    data.raise_for_status()\n",
    "    return data.json()\n",
    "\n",
    "def compare_ver(a, b):\n",
    "    \"\"\" Logically compare two Firefox version strings. Split the string into\n",
    "    pieces, and compare each piece numerically.\n",
    "\n",
    "    Returns -1, 0, or 1 depending on whether a is less than, equal to, or\n",
    "    greater than b.\n",
    "    \"\"\"\n",
    "    if a == b:\n",
    "        return 0\n",
    "\n",
    "    ap = [int(p) for p in a.split(\".\")]\n",
    "    bp = [int(p) for p in b.split(\".\")]\n",
    "    lap = len(ap)\n",
    "    lbp = len(bp)\n",
    "\n",
    "    # min # of pieces\n",
    "    mp = lap\n",
    "    if lbp < mp:\n",
    "        mp = lbp\n",
    "\n",
    "    for i in range(mp):\n",
    "        if ap[i] < bp[i]:\n",
    "            return -1\n",
    "        if ap[i] > bp[i]:\n",
    "            return 1\n",
    "\n",
    "    if lap > lbp:\n",
    "        # a has more pieces, common pieces are the same, a is greater\n",
    "        return 1\n",
    "\n",
    "    if lap < lbp:\n",
    "        # a has fewer pieces, common pieces are the same, b is greater\n",
    "        return -1\n",
    "\n",
    "    # They are exactly the same.\n",
    "    return 0\n",
    "\n",
    "def get_release_info():\n",
    "    \"\"\" Fetch information about Firefox release dates. Returns an object\n",
    "    containing two sections:\n",
    "\n",
    "    'major' - contains major releases (i.e. 41.0)\n",
    "    'minor' - contains stability releases (i.e. 41.0.1)\n",
    "    \"\"\"\n",
    "    major_info = fetch_json(\"https://product-details.mozilla.org/1.0/firefox_history_major_releases.json\")\n",
    "    if major_info is None:\n",
    "        raise Exception(\"Failed to fetch major version info\")\n",
    "    minor_info = fetch_json(\"https://product-details.mozilla.org/1.0/firefox_history_stability_releases.json\")\n",
    "    if minor_info is None:\n",
    "        raise Exception(\"Failed to fetch minor version info\")\n",
    "    return {\"major\": major_info, \"minor\": minor_info}\n",
    "\n",
    "def daynum_to_date(daynum):\n",
    "    \"\"\" Convert a number of days to a date. If it's out of range, default to a max date.\n",
    "    :param daynum: A number of days since Jan 1, 1970\n",
    "    \"\"\"\n",
    "    if daynum is None:\n",
    "        return None\n",
    "    if daynum < 0:\n",
    "        return None\n",
    "    daycount = int(daynum)\n",
    "    if daycount > 1000000:\n",
    "        # Some time in the 48th century, clearly bogus.\n",
    "        daycount = 1000000\n",
    "    return date(1970, 1, 1) + timedelta(daycount)\n",
    "\n",
    "def sane_date(d):\n",
    "    \"\"\" Check if the given date looks like a legitimate time on which activity\n",
    "    could have happened.\n",
    "    \"\"\"\n",
    "    if d is None:\n",
    "        return False\n",
    "    return d > date(2000, 1, 1) and d < _datetime.utcnow().date() + timedelta(2)\n",
    "\n",
    "def is_funnelcake(distro):\n",
    "    \"\"\" Check if a given distribution_id appears to be a funnelcake build.\"\"\"\n",
    "    if distro is None:\n",
    "        return False\n",
    "    return funnelcake_pattern.match(distro) is not None\n",
    "\n",
    "top_countries = set([\"US\", \"DE\", \"FR\", \"RU\", \"BR\", \"IN\", \"PL\", \"ID\", \"GB\", \"CN\",\n",
    "                  \"IT\", \"JP\", \"CA\", \"ES\", \"UA\", \"MX\", \"AU\", \"VN\", \"EG\", \"AR\",\n",
    "                  \"PH\", \"NL\", \"IR\", \"CZ\", \"HU\", \"TR\", \"RO\", \"GR\", \"AT\", \"CH\"])\n",
    "\n",
    "def top_country(country):\n",
    "    global top_countries\n",
    "    if(country in top_countries):\n",
    "        return country\n",
    "    return \"ROW\"\n",
    "\n",
    "def get_week_num(creation, today):\n",
    "    if creation is None or today is None:\n",
    "        return None\n",
    "\n",
    "    diff = (today.date() - creation).days\n",
    "    if diff < 0:\n",
    "        # Creation date is in the future. Bad data :(\n",
    "        return -1\n",
    "    # The initial week is week zero.\n",
    "    return int(diff / 7)\n",
    "\n",
    "# The number of seconds in a single hour, casted to float, so we get the fractional part\n",
    "# when converting.\n",
    "SECONDS_IN_HOUR = float(60 * 60)\n",
    "\n",
    "def convert(d2v, week_start, datum):\n",
    "    out = {\"good\": False}\n",
    "\n",
    "    pcd = daynum_to_date(datum.profile_creation_date)\n",
    "    if not sane_date(pcd):\n",
    "        return out\n",
    "\n",
    "    pcd_formatted = _datetime.strftime(pcd, \"%Y-%m-%d\")\n",
    "\n",
    "    out[\"client_id\"] = datum.client_id\n",
    "    channel = datum.normalized_channel\n",
    "    out[\"is_funnelcake\"] = is_funnelcake(datum.distribution_id)\n",
    "    if out[\"is_funnelcake\"]:\n",
    "        channel = \"{}-cck-{}\".format(datum.normalized_channel, datum.distribution_id)\n",
    "    out[\"channel\"] = channel\n",
    "    out[\"geo\"] = top_country(datum.country)\n",
    "    out[\"acquisition_period\"] = snap_to_beginning_of_week(pcd, \"Sunday\")\n",
    "    out[\"start_version\"] = get_effective_version(d2v, channel, pcd_formatted)\n",
    "    \n",
    "    # bug 1337037 - stub attribution\n",
    "    attribution_fields = [\"source\", \"medium\", \"campaign\", \"content\"]\n",
    "    if datum.attribution:\n",
    "        for field in attribution_fields:\n",
    "            out[field] = datum.attribution[field]\n",
    "    \n",
    "    # bug 1323598\n",
    "    out['distribution_id'] = datum.distribution_id\n",
    "    out['default_search_engine'] = datum.default_search_engine\n",
    "    out['locale'] = datum.locale\n",
    "\n",
    "    deviceCount = 0\n",
    "    if datum.sync_count_desktop is not None:\n",
    "        deviceCount += datum.sync_count_desktop\n",
    "    if datum.sync_count_mobile is not None:\n",
    "        deviceCount += datum.sync_count_mobile\n",
    "            \n",
    "    if deviceCount > 1:\n",
    "        out[\"sync_usage\"] = \"multiple\"\n",
    "    elif deviceCount == 1:\n",
    "        out[\"sync_usage\"] = \"single\"\n",
    "    elif datum.sync_configured is not None:\n",
    "        if datum.sync_configured:\n",
    "            out[\"sync_usage\"] = \"single\"\n",
    "        else:\n",
    "            out[\"sync_usage\"] = \"no\"\n",
    "    # Else we don't set sync_usage at all, and use a default value later.\n",
    "    \n",
    "    out[\"current_version\"] = datum.version\n",
    "    \n",
    "    # The usage time is in seconds, but we really need hours.\n",
    "    # Because we filter out broken subsession_lengths, we could end up with clients with no\n",
    "    # usage hours.\n",
    "    out[\"usage_hours\"] = (datum.usage_seconds / SECONDS_IN_HOUR) if datum.usage_seconds is not None else 0.0\n",
    "    out[\"squared_usage_hours\"] = out[\"usage_hours\"] ** 2\n",
    "    \n",
    "    # Incoming subsession_start_date looks like \"2016-02-22T00:00:00.0-04:00\"\n",
    "    client_date = None\n",
    "    if datum.subsession_start_date is not None:\n",
    "        try:\n",
    "            client_date = _datetime.strptime(datum.subsession_start_date[0:10], \"%Y-%m-%d\")\n",
    "        except ValueError as e1:\n",
    "            # Bogus format\n",
    "            pass\n",
    "        except TypeError as e2:\n",
    "            # String contains null bytes or other weirdness. Example:\n",
    "            # TypeError: must be string without null bytes, not unicode\n",
    "            pass\n",
    "    if client_date is None:\n",
    "        # Fall back to submission date\n",
    "        client_date = _datetime.strptime(datum.submission_date_s3, \"%Y%m%d\")\n",
    "    out[\"current_week\"] = get_week_num(pcd, client_date)\n",
    "    out[\"is_active\"] = \"yes\"\n",
    "    if client_date is not None:\n",
    "        try:\n",
    "            if _datetime.strftime(client_date, \"%Y%m%d\") < week_start:\n",
    "                out[\"is_active\"] = \"no\"\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "    out[\"good\"] = True\n",
    "    return out\n",
    "\n",
    "def csv(f):\n",
    "    return \",\".join([unicode(a) for a in f])\n",
    "\n",
    "# Build the \"effective version\" cache:\n",
    "d2v = make_d2v(get_release_info())\n",
    "\n",
    "def get_churn_filename(week_start, week_end):\n",
    "    return \"churn-{}-{}.by_activity.csv.gz\".format(week_start, week_end)\n",
    "\n",
    "def fmt(d, date_format=\"%Y%m%d\"):\n",
    "    return _datetime.strftime(d, date_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the aggregates\n",
    "\n",
    "Run the aggregation code, detecting files that are missing.\n",
    "\n",
    "The fields we want in the output are:\n",
    " - channel (appUpdateChannel)\n",
    " - geo (bucketed into top 30 countries + \"rest of world\")\n",
    " - is_funnelcake (contains \"-cck-\"?)\n",
    " - acquisition_period (cohort_week)\n",
    " - start_version (effective version on profile creation date)\n",
    " - sync_usage (\"no\", \"single\" or \"multiple\" devices)\n",
    " - current_version (current appVersion)\n",
    " - current_week (week)\n",
    " - is_active (were the client_ids active this week or not)\n",
    " - n_profiles (count of matching client_ids)\n",
    " - usage_hours (sum of the per-client subsession lengths, clamped in the [0, MAX_SUBSESSION_LENGTH] range)\n",
    " - sum_squared_usage_hours (the sum of squares of the usage hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "\n",
    "MAX_SUBSESSION_LENGTH = 60 * 60 * 48 # 48 hours in seconds.\n",
    "\n",
    "record_columns = [\n",
    "    'channel',\n",
    "    'geo',\n",
    "    'is_funnelcake',\n",
    "    'acquisition_period',\n",
    "    'start_version',\n",
    "    'sync_usage',\n",
    "    'current_version',\n",
    "    'current_week',\n",
    "    'source',\n",
    "    'medium',\n",
    "    'campaign',\n",
    "    'content',\n",
    "    'distribution_id',\n",
    "    'default_search_engine',\n",
    "    'locale',\n",
    "    'is_active',\n",
    "    'n_profiles',\n",
    "    'usage_hours',\n",
    "    'sum_squared_usage_hours'\n",
    "]\n",
    "\n",
    "\n",
    "def get_newest_per_client(df):\n",
    "    windowSpec = Window.partitionBy(F.col('client_id')).orderBy(F.col('timestamp').desc())\n",
    "    rownum_by_timestamp = (F.row_number().over(windowSpec))\n",
    "    selectable_by_client = df.select(\n",
    "        rownum_by_timestamp.alias('row_number'),\n",
    "        *[F.col(col) for col in df.columns]\n",
    "    )\n",
    "    return selectable_by_client.filter(selectable_by_client['row_number'] == 1)\n",
    "\n",
    "\n",
    "def upload_to_s3(records, churn_outfile):\n",
    "    client = boto3.client('s3', 'us-west-2')\n",
    "    transfer = S3Transfer(client)\n",
    "    churn_bucket = \"net-mozaws-prod-us-west-2-pipeline-analysis\"\n",
    "    churn_filepath = \"amiyaguchi/churn\"\n",
    "\n",
    "    print \"{}: Writing output to {}\".format(_datetime.utcnow(), churn_outfile)\n",
    "\n",
    "    # Write the file out as gzipped csv\n",
    "    with gzip.open(churn_outfile, 'wb') as fout:\n",
    "        fout.write(\",\".join(record_columns) + \"\\n\")\n",
    "        print \"{}: Wrote header to {}\".format(_datetime.utcnow(), churn_outfile)\n",
    "        for r in records:\n",
    "            try:\n",
    "                fout.write(csv(r))\n",
    "                fout.write(\"\\n\")\n",
    "            except UnicodeEncodeError as e:\n",
    "                print(\"{}: Error writing line: {} // {}\"\n",
    "                      .format(_datetime.utcnow(), e, r))\n",
    "        print \"{}: finished writing lines\".format(_datetime.utcnow())\n",
    "\n",
    "    # Now upload it to S3:\n",
    "    churn_s3 = \"{}/{}\".format(churn_filepath, churn_outfile)\n",
    "    transfer.upload_file(churn_outfile, churn_bucket, churn_s3,\n",
    "                         extra_args={'ACL': 'bucket-owner-full-control'})\n",
    "\n",
    "    # TODO: Re-enable uploading to the dashboard when cutover happends\n",
    "    ENABLE_UPLOAD_DASHBOARD = False\n",
    "\n",
    "    if ENABLE_UPLOAD_DASHBOARD:\n",
    "        # Also upload it to the dashboard:\n",
    "        # Update the dashboard file\n",
    "        dash_bucket = \"net-mozaws-prod-metrics-data\"\n",
    "        dash_s3_name = \"telemetry-churn/{}\".format(churn_outfile)\n",
    "        transfer.upload_file(churn_outfile, dash_bucket, dash_s3_name,\n",
    "                             extra_args={'ACL': 'bucket-owner-full-control'})\n",
    "\n",
    "    \n",
    "def compute_churn_week(df, week_start, bucket, prefix, enable_upload_csv=False):\n",
    "    \"\"\"Compute the churn data for this week. Note that it takes 10 days\n",
    "    from the end of this period for all the activity to arrive. This data\n",
    "    should be from Sunday through Saturday.\n",
    "    \n",
    "    df: DataFrame of the dataset relevant to computing the churn\n",
    "    week_start: datestring of this time period\n",
    "    enable_upload_to_s3: bool that determines whether a gzipped csv file is uploaded\"\"\"\n",
    "    \n",
    "    week_start_date = _datetime.strptime(week_start, \"%Y%m%d\")\n",
    "    week_end_date = week_start_date + timedelta(6)\n",
    "    week_start = fmt(week_start_date)\n",
    "    week_end = fmt(week_end_date)\n",
    "\n",
    "    # Verify that the start date is a Sunday\n",
    "    if week_start_date.weekday() != 6:\n",
    "        print(\"Week start date {} is not a Sunday\".format(week_start))\n",
    "        return\n",
    "\n",
    "    # If the data for this week can still be coming, don't try to compute the churn.\n",
    "    week_end_slop = fmt(week_end_date + timedelta(10))\n",
    "    today = fmt(_datetime.utcnow())\n",
    "    if week_end_slop >= today:\n",
    "        print(\"Skipping week of {} to {} - Data is still arriving until {}.\"\n",
    "              .format(week_start, week_end, week_end_slop))\n",
    "        return\n",
    "\n",
    "    print(\"Starting week from {} to {} at {}\"\n",
    "          .format(week_start, week_end, _datetime.utcnow()))\n",
    "    # the subsession_start_date field has a different form than submission_date_s3,\n",
    "    # so needs to be formatted with hyphens.\n",
    "    week_end_excl = fmt(week_end_date + timedelta(1), date_format=\"%Y-%m-%d\")\n",
    "    week_start_hyphenated = fmt(week_start_date, date_format=\"%Y-%m-%d\")\n",
    "\n",
    "    current_week = (\n",
    "        df.filter(df['submission_date_s3'] >= week_start)\n",
    "          .filter(df['submission_date_s3'] <= week_end_slop)\n",
    "          .filter(df['subsession_start_date'] >= week_start_hyphenated)\n",
    "          .filter(df['subsession_start_date'] < week_end_excl)\n",
    "    )\n",
    "\n",
    "    # Clamp broken subsession values in the [0, MAX_SUBSESSION_LENGTH] range.\n",
    "    clamped_subsession = (\n",
    "        current_week\n",
    "        .select(\n",
    "            F.col('client_id'),\n",
    "            F.when(\n",
    "                F.col('subsession_length') > MAX_SUBSESSION_LENGTH, MAX_SUBSESSION_LENGTH\n",
    "            ).otherwise(\n",
    "                F.when(F.col('subsession_length') < 0, 0).otherwise(F.col('subsession_length'))\n",
    "            ).alias('subsession_length')\n",
    "            )\n",
    "    )\n",
    "\n",
    "    # Compute the overall usage time for each client by summing the subsession lengths.\n",
    "    grouped_usage_time = (\n",
    "        clamped_subsession\n",
    "            .groupby('client_id')\n",
    "            .sum('subsession_length')\n",
    "            .withColumnRenamed('sum(subsession_length)', 'usage_seconds')\n",
    "    )\n",
    "\n",
    "    # Get the newest ping per client and append to original dataframe\n",
    "    newest_per_client = get_newest_per_client(current_week)\n",
    "    newest_with_usage = newest_per_client.join(grouped_usage_time, 'client_id', 'inner')\n",
    "\n",
    "    converted = newest_with_usage.rdd.map(lambda x: convert(d2v, week_start, x))\n",
    "\n",
    "    # Don't bother to filter out non-good records - they will appear \n",
    "    # as 'unknown' in the output.\n",
    "    countable = converted.map(\n",
    "        lambda x: (\n",
    "            (\n",
    "                # attributes unique to a client\n",
    "                x.get('channel', 'unknown'),\n",
    "                x.get('geo', 'unknown'),\n",
    "                \"yes\" if x.get('is_funnelcake', False) else \"no\",\n",
    "                _datetime.strftime(x.get('acquisition_period', date(2000, 1, 1)), \"%Y-%m-%d\"),\n",
    "                x.get('start_version', 'unknown'),\n",
    "                x.get('sync_usage', 'unknown'),\n",
    "                x.get('current_version', 'unknown'),\n",
    "                x.get('current_week', 'unknown'),\n",
    "                x.get('source', 'unknown'),\n",
    "                x.get('medium', 'unknown'),\n",
    "                x.get('campaign', 'unknown'),\n",
    "                x.get('content', 'unknown'),\n",
    "                x.get('distribution_id', 'unknown'),\n",
    "                x.get('default_search_engine', 'unknown'),\n",
    "                x.get('locale', 'unknown'),\n",
    "                x.get('is_active', 'unknown')\n",
    "            ),(\n",
    "                1,  # active users \n",
    "                x.get('usage_hours', 0),\n",
    "                x.get('squared_usage_hours',0) \n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def reduce_func(x, y):\n",
    "        return (x[0] + y[0], # Sum active users\n",
    "                x[1] + y[1], # Sum usage_hours\n",
    "                x[2] + y[2]) # Sum squared_usage_hours\n",
    "\n",
    "    aggregated = countable.reduceByKey(reduce_func)\n",
    "\n",
    "    records_df = aggregated.map(lambda x: x[0] + x[1]).toDF(record_columns)\n",
    "\n",
    "    # New jobs will read as parquet, csv files exist for legacy purposes\n",
    "    if enable_upload_csv:\n",
    "        # TODO: groupby sum using the original set of columns\n",
    "        churn_outfile = get_churn_filename(week_start, week_end)\n",
    "        # Don't bother with replacing any csv files that already exist\n",
    "        try:\n",
    "            upload_to_s3(records_df.rdd.collect(), churn_outfile)\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            print(\"File for {} already exists, skipping upload: {}\"\n",
    "                  .format(churn_outfile, e))\n",
    "\n",
    "    # Write to s3 as parquet, file size is on the order of 40MB. We bump the version\n",
    "    # number because v1 is the path to the old telemetry-batch-view churn data.\n",
    "    parquet_s3_path = (\"s3://{}/{}/week_start={}\"\n",
    "                       .format(bucket, prefix, week_start))\n",
    "    print \"{}: Writing output as parquet to {}\".format(_datetime.utcnow(), parquet_s3_path)\n",
    "    records_df.repartition(1).write.parquet(parquet_s3_path, mode=\"overwrite\")\n",
    "\n",
    "    print(\"Finished week from {} to {} at {}\"\n",
    "          .format(week_start, week_end, _datetime.utcnow()))\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)//7):\n",
    "        yield (start_date + timedelta(n*7)).strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "def backfill(df, start_date_yyyymmdd, bucket, prefix, enable_upload=False):\n",
    "    \"\"\" Import data from a start date to an end date\"\"\"\n",
    "    start_date = snap_to_beginning_of_week(\n",
    "        _datetime.strptime(start_date_yyyymmdd, \"%Y%m%d\"), \n",
    "        \"Sunday\")\n",
    "    end_date = _datetime.utcnow() - timedelta(1) # yesterday\n",
    "    for d in daterange(start_date, end_date):\n",
    "        try:\n",
    "            compute_churn_week(df, d, bucket, prefix, enable_upload)\n",
    "        except Exception as e:\n",
    "            print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "manual = False\n",
    "if manual:\n",
    "    os.environ['date'] = fmt(_datetime.now())\n",
    "    os.environ['bucket'] = 'net-mozaws-prod-us-west-2-pipeline-analysis'\n",
    "    os.environ['prefix'] = 'amiyaguchi/churn'\n",
    "\n",
    "env_date = environ.get('date')\n",
    "if not env_date:\n",
    "    raise ValueError(\"date not provided\")\n",
    "bucket = environ.get('bucket', 'telemetry-parquet')\n",
    "prefix = environ.get('prefix', 'churn')\n",
    "\n",
    "# If this job is scheduled, we need the input date to lag a total of \n",
    "# 10 days of slack for incoming data + the 7 days used in churn computation, or 17 days\n",
    "# from the current date. \n",
    "week_start_date = snap_to_beginning_of_week(\n",
    "    _datetime.strptime(env_date, \"%Y%m%d\") - timedelta(17),\n",
    "    \"Sunday\")\n",
    "\n",
    "compute_churn_week(df = dataset,\n",
    "                   week_start = fmt(week_start_date),\n",
    "                   bucket = bucket,\n",
    "                   prefix = prefix,\n",
    "                   enable_upload_csv = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 20151101 is world start, but data is only stored for 9 months on main_summary\n",
    "# Uncomment to manually backfill this job\n",
    "# backfill(dataset, '20170101', bucket, prefix, False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
