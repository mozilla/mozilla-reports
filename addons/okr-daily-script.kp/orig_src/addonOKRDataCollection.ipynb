{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: addon_aggregates derived dataset script\n",
    "authors:\n",
    "- Ben Miroglio\n",
    "tags:\n",
    "- add-ons\n",
    "- okr\n",
    "- derived dataset\n",
    "created_at: 2017-02-08 00:00:00\n",
    "updated_at: 2017-02-15\n",
    "tldr: script to be run daily that contructs the addon_aggregates table in re:dash\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-ons 2017 OKR Data Collection\n",
    "\n",
    "Some OKRs for 2017 can be feasibly collected via the `addons` and `main_summary` tables. These tables are huge and aren't appropriate to query directly via re:dash. This script condenses these tables so that the result contains the least data possible to track the following OKRs:\n",
    "\n",
    "* **OKR 1: Increase number of users who self-install an Add-on by 5%**\n",
    "* **OKR 2: Increase average number of add-ons per profile by 3%**\n",
    "* **OKR 3: Increase number of new Firefox users who install an add-on in first 14 days by 25%**\n",
    "\n",
    "These OKRs, in addition to other add-on metrics, are tracked via the [Add-on OKRs Dashboard](https://sql.telemetry.mozilla.org/dashboard/add-on-okrs_1#edit_dashboard_dialog) in re:dash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pyspark.sql.functions as fun\n",
    "import pyspark.sql.types as st\n",
    "import math\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_repartition(df, record_size, partition_size=280):\n",
    "    '''\n",
    "    Repartitions a spark DataFrame <df> so that each partition is \n",
    "    ~ <partition_size>MB, defaulting to 280MB. record_size must be \n",
    "    estimated beforehand--i.e. write the dataframe to s3, get the size \n",
    "    in bytes and divide by df.count(). \n",
    "    \n",
    "    Returns repartitioned dataframe if a repartition is necessary.\n",
    "    '''\n",
    "    total_records = df.count()\n",
    "    print \"-- Found {} records\".format(total_records),\n",
    "    \n",
    "    #convert megabytes to bytes\n",
    "    partition_size *= 1000000\n",
    "    \n",
    "    records_per_partition = partition_size / record_size\n",
    "    num_partitions = int(math.ceil(total_records / records_per_partition))\n",
    "\n",
    "    if num_partitions != df.rdd.getNumPartitions():\n",
    "        print \"-- Repartitioning with {} partitions\".format(num_partitions)\n",
    "        df = df.repartition(num_partitions)\n",
    "    return df\n",
    "\n",
    "def get_env_date():\n",
    "    '''\n",
    "    Returns environment date if it exists.\n",
    "    otherwise returns yesterday's date\n",
    "    '''\n",
    "    yesterday = dt.datetime.strftime(dt.datetime.utcnow() - dt.timedelta(1), \"%Y%m%d\")\n",
    "    return os.environ.get('date', yesterday)\n",
    "\n",
    "def get_dest(bucket, prefix):\n",
    "    '''\n",
    "    Uses environment bucket if it exists.\n",
    "    Otherwises uses the bucket passed as a parameter\n",
    "    '''\n",
    "    bucket = os.environ.get('bucket', bucket)\n",
    "    return '/'.join([bucket, prefix])\n",
    "    \n",
    "\n",
    "# I use -1 and 1 because it allows me to segment users \n",
    "# into three groups for two different cases:\n",
    "#\n",
    "# **Case 1**: \n",
    "# Users that have only foreign-installed add-ons, only self-installed add-ons, \n",
    "# or a combination. Applying `boot_to_int()` on a `foreign_install` boolean, \n",
    "# I can sum the resulting field grouped by `client_id` and `submission_date_s3`  \n",
    "# to identify these groups as 1, -1, and 0 respectively.\n",
    "#\n",
    "# **Case 2**: Users that have the default theme, a custom theme, \n",
    "# or changed their theme (from default to custom or visa versa) on a given day: \n",
    "# Applying `boot_to_int()` on a `has_custom_theme` boolean, I can sum the \n",
    "# resulting field grouped by `client_id` and `submission_date_s3`  \n",
    "# to identify these groups as -1, 1, and 0 respectively.\n",
    "bool_to_int = fun.udf(lambda x: 1 if x == True else -1, st.IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless specified in the environment, the target date is yesterday, and the bucket used is passed as a string to `get_dest()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_date = get_env_date()\n",
    "dest = get_dest(bucket=\"telemetry-parquet\", prefix=\"addons/agg/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `addons` and `main_summary` for yesterday (unless specified in the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addons = sqlContext.read.parquet(\"s3://telemetry-parquet/addons/v2\")\n",
    "addons = addons.filter(addons.submission_date_s3 == target_date) \\\n",
    "               .filter(addons.is_system == False) \\\n",
    "               .filter(addons.user_disabled == False) \\\n",
    "               .filter(addons.app_disabled == False) \\\n",
    "\n",
    "ms = sqlContext.read.option('mergeSchema', 'true')\\\n",
    "             .parquet('s3://telemetry-parquet/main_summary/v3')\n",
    "ms = ms.filter(ms.submission_date_s3 == target_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the aggregations / joins that we **don't** want to do in re:dash.\n",
    "\n",
    "* The resulting table is one row per distinct client, day, channel, and install type\n",
    "  + foreign_install = true -> side-loaded add-on, foreign_install = false ->  self-installed add-on\n",
    "* Each client has a static field for profile_creation_date and min_install_day (earliest add-on installation date)\n",
    "* Each client has a daily field `user_type`\n",
    "   * 1  -> only foreign installed add-ons\n",
    "   * -1 -> only self-installed\n",
    "   * 0  -> foreign installed and self installed\n",
    "* Each client has a daily field `has_custom_theme`.\n",
    "   * 1  -> has a custom theme\n",
    "   * -1 -> has default theme\n",
    "   * 0  -> changed from default to custom on this date\n",
    "* To facilitate total population percentages, each submission date/channel has two static fields\n",
    "  + n_custom_theme_clients (# distinct clients on that day/channel with a custom theme)\n",
    "  + n_clients (# distinct total clients on that date/channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "default_theme_id = \"{972ce4c6-7e08-4474-a285-3208198ce6fd}\"\n",
    "\n",
    "\n",
    "# count of distinct client submission_date, channel and install type\n",
    "count_by_client_day = addons\\\n",
    "  .select(['client_id', 'submission_date_s3', 'normalized_channel',\n",
    "           'foreign_install', 'addon_id'])\\\n",
    "  .distinct()\\\n",
    "  .groupBy(['client_id', 'submission_date_s3','foreign_install', 'normalized_channel'])\\\n",
    "  .count()\n",
    "\n",
    "# count of clients that have only foreign_installed, only self_installed and both\n",
    "# per day/channel\n",
    "user_types = count_by_client_day\\\n",
    "  .select(['client_id', 'submission_date_s3', 'normalized_channel',\n",
    "           bool_to_int('foreign_install').alias('user_type')])\\\n",
    "  .groupBy(['client_id', 'submission_date_s3', 'normalized_channel'])\\\n",
    "  .sum('user_type')\\\n",
    "  .withColumnRenamed('sum(user_type)', 'user_type')\n",
    "\n",
    "count_by_client_day = count_by_client_day.join(user_types, \n",
    "                                               on=['client_id', 'submission_date_s3', 'normalized_channel'])\n",
    "\n",
    "\n",
    "# does a client have a custom theme?\n",
    "# aggregate distinct values on a day/channel, since a client could have\n",
    "# changed from default to custom\n",
    "ms_has_theme = ms.select(\\\n",
    "   ms.client_id, ms.normalized_channel, bool_to_int(ms.active_theme.addon_id != default_theme_id).alias('has_custom_theme'))\\\n",
    "  .distinct()\\\n",
    "  .groupBy(['client_id', 'normalized_channel']).sum('has_custom_theme') \\\n",
    "  .withColumnRenamed('sum(has_custom_theme)', 'has_custom_theme')\n",
    "               \n",
    "\n",
    "# client_id, profile_creation_date and the earliest\n",
    "# install day for an addon\n",
    "ms_install_days = ms\\\n",
    "  .select(['client_id', 'profile_creation_date', \n",
    "           fun.explode('active_addons').alias('addons')])\\\n",
    "  .groupBy(['client_id', 'profile_creation_date'])\\\n",
    "  .agg(fun.min(\"addons.install_day\").alias('min_install_day'))\n",
    "    \n",
    "\n",
    "# combine data\n",
    "current = count_by_client_day\\\n",
    "  .join(ms_install_days, on='client_id', how='left')\\\n",
    "  .join(ms_has_theme, on=['client_id', 'normalized_channel'], how='left')\\\n",
    "  .drop('submission_date_s3')\n",
    "\n",
    "\n",
    "# add total number of distinct clients per day/channel\n",
    "# and total number of distinct clients with a custom theme per day/channel\n",
    "# Note that we could see the same client on multiple channels\n",
    "# so downstream analysis should be done within channel\n",
    "n_clients = ms.select(['client_id', 'normalized_channel']).distinct()\\\n",
    "               .groupby('normalized_channel').count()\\\n",
    "               .withColumnRenamed('count', 'n_clients')\n",
    "\n",
    "n_custom_themes = ms_has_theme\\\n",
    "  .filter(ms_has_theme.has_custom_theme >= 0)\\\n",
    "  .select(['client_id', 'normalized_channel']).distinct()\\\n",
    "  .groupby('normalized_channel').count()\\\n",
    "  .withColumnRenamed('count', 'n_custom_theme_clients')\n",
    "\n",
    "current = current.join(n_custom_themes, on='normalized_channel')\\\n",
    "                 .join(n_clients, on='normalized_channel')\n",
    "    \n",
    "current = current.withColumn('n_clients', current.n_clients.cast(st.IntegerType()))\\\n",
    "                 .withColumn('n_custom_theme_clients', current.n_custom_theme_clients.cast(st.IntegerType()))\n",
    "    \n",
    "# repartition data\n",
    "current = optimize_repartition(current, record_size=39)\n",
    "\n",
    "# write to s3\n",
    "current.write.format(\"parquet\")\\\n",
    "  .save('s3://' + dest + '/submission_date_s3={}'.format(target_date), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current.printSchema()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}